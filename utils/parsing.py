import os
import sys
from tempfile import TemporaryDirectory
from typing import List, Optional, Union

from tap import Tap

from utils.utils import create_exp_name, find_unique_name, makedirs

TEMP_DIR = None


class DatasetArguments(Tap):
    dataset: str = "superuser_askubuntu"  # The dataset to load

    def add_arguments(self) -> None:
        self.add_argument(
            "--dataset",
            choices=["superuser_askubuntu", "multinews", "snli", "multirc",],
        )


class SharedArguments(Tap):
    # General arguments
    dataset: str = None  # Will be set by DatasetArguments
    task: str = "classify"
    exp_name: str = None  # Will be autogenerated based on command line arguments
    log_dir: Optional[str] = None  # Directory where logs will be saved
    checkpoint_path: Optional[str] = None  # Path to checkpoint .pt file to load
    gpu: int = 0  # Which gpu to use
    small_data: bool = False  # Whether to load only a small amount of data for debugging
    preds_dir: Optional[
        str
    ] = None  # Path to directory where predictions will be saved as a .pkl file
    viz_dir: Optional[
        str
    ] = "../viz/test"  # Path to directory where visualizations will be saved
    num_predict: int = 500  # The number of examples to predict/visualize
    log_frequency: int = 20  # Frequency of logging to console and tensorboard
    test_every: int = 1
    encoder: str = "sru"
    result_dir: str = ""
    cost_regulate: float = 0.0
    regu_type: str = "l1"

    embedder_pooling: str = "average"
    attention_heads: int = 4
    attention_units: int = 64
    save_every: int = 0

    # Alignment arguments
    epsilon: float = 1e-4  # Epsilon weighting value for entropy in sinkhorn algorithm
    unbalanced_lambda: Optional[
        float
    ] = None  # Lambda factor for marginals for unbalanced OT
    cost_fn: str = "cosine_distance"  # Value function to use when comparing sentences
    alignment: str = "sinkhorn"  # Alignment method, (average, all_pairs, attention, or sinkhorn)
    one_to_k: Optional[
        int
    ] = None  # If not None, this is k for one-to-k sinkhorn alignment; if k is -1, uses floor(max(n, m) / min(n, m))
    max_num_aligned: Optional[
        int
    ] = None  # Maximum number of sentences to align in sinkhorn
    optional_alignment: bool = False  # Allow any sentence to align to a dummy node; requires cost that can be + or -
    split_dummy: bool = False  # Whether to split the dummy into multiple nodes with 1 marginal each
    order_lambda: float = 0.0  # Weight for diagonal order preserving cost (0 = no weight, 1 = all weight)
    error_check_frequency: int = 10
    error_threshold: float = 1e-6

    word_to_word: bool = False  # if to align
    train_rationale: bool = False
    rationale_weight: float = 1.0
    # SRU:
    embedding_path: str = "/persist/workspace/embeddings/cc.en.300.bin"  # Path to file containing pretrained word embeddings
    num_layers: int = 1  # Number of RNN layers
    hidden_size: int = 300  # Hidden size of the network
    dropout: float = 0.2  # Dropout
    attend_l: int = 1  # number of layer for attentive feedforward in decomposable attention model
    compare_l: int = 1  # number of layer for compare feedforward in decomposable attention model
    aggregate_l: int = 1  # number of layer for aggregate feedforward in decomposable attention model
    ffn_hidden_size: int = 300  # hiddensize in decomposable attention model
    attend_activation: str = "relu"
    # ffn_dropout: float = 0.2  # dropout in decomposable attention model
    stop_cls_step: int = 10000  # at some epoch stop training classifer

    # Attention
    no_sentence_tokenize: bool = False  # Whether to treat the text as one string rather than averaging sentence embeddings
    attention_type: int = 5  # Lazy way of defining which attention model to use
    attention_temp: float = 1.0  # Adding  term to make the attention peakier
    sparsity_thresholds: List[float] = [
        0.0,
        0.01,
        0.1,
        1,
        2,
    ]  # , 1] #[0.0, 0.001, 0.1, 1, 10]  # The threshold (times 1/nm) below which values are masked out during evaluation
    absolute_threshold: bool = False
    step_aneal: int = 1000
    ratio_threshold: bool = False
    using_sparsemax: bool = False
    force_attention_linear: bool = False
    transitive: bool = False
    # Bert
    bert: bool = False  # if use bert as encoder
    bert_type: str = "roberta-base"  # which pretrained model to use
    gradient_accumulation_steps: int = 1
    warmup_steps: int = 100  # leanrnig rate decay method
    warmup_ratio: float = 0.06
    max_batches_per_epoch: int = 5000000000
    bert_batch_size: int = 4
    num_positives: Optional[int] = None  # Number of positives sampled for each document
    num_negatives: Optional[int] = 20  # Number of negatives sampled for each document
    num_eval_positives: Optional[
        int
    ] = None  # Number of dev positives sampled for each document
    num_eval_negatives: Optional[
        int
    ] = None  # Number of dev negatives sampled for each document
    max_num_sentences: int = 100  # Maximum number of sentences per document
    max_sentence_length: int = 80  # Maximum number of tokens in a sentence
    # max_text_length: int = 80

    # Training arguments
    batch_size: int = 50  # Batch size
    lr: float = 1e-3  # Learning rate
    weight_decay: float = 1e-6  # Weight decay
    gamma: float = 0.98  # Exponential decay of learning rate scheduler
    epochs: int = 8  # Number of epochs
    margin: float = 0.1  # Margin for hinge loss
    lr_decay_method: str = "noam"  # leanrnig rate decay method
    loss_fn: str = "hinge"
    hinge_pooling: str = "smoothmax"  # Type of pooling for hinge loss
    hinge_alpha: float = 0.1  # Type of pooling for hinge loss
    train_rationale_till: int = 20

    def add_arguments(self) -> None:
        self.add_argument(
            "--cost_fn",
            choices=[
                "euclidean",
                "euclidean_normalized",
                "dot_product",
                "scaled_dot_product",
                "cosine_similarity",
                "cosine_distance",
            ],
        )
        self.add_argument(
            "--alignment", choices=["average", "all_pairs", "attention", "sinkhorn"],
        )
        self.add_argument(
            "--hinge_pooling", choices=["max", "average", "smoothmax", "none"]
        )

    def process_args(self) -> None:
        # Create experiment name
        if self.exp_name is None:
            self.exp_name = create_exp_name(
                flags_and_values=sys.argv[1:],
                flag_skip_set={"dataset"},
                skip_paths=True,
            )

        # Set log directory
        if self.log_dir is None:
            global TEMP_DIR
            TEMP_DIR = TemporaryDirectory()
            self.log_dir = TEMP_DIR.name
        else:
            self.log_dir = find_unique_name(
                os.path.join(self.log_dir, self.dataset, self.exp_name)
            )
            makedirs(self.log_dir)

        # Set preds directory
        if self.preds_dir is not None:
            self.preds_dir = find_unique_name(
                os.path.join(self.preds_dir, self.dataset, self.exp_name)
            )
            makedirs(self.preds_dir)

        # Set viz directory
        if self.viz_dir is not None:
            self.viz_dir = find_unique_name(
                os.path.join(self.viz_dir, self.dataset, self.exp_name)
            )
            makedirs(self.viz_dir)

        # Special cases of alignment
        special_sinkhorn = (
            self.one_to_k is not None
            or self.max_num_aligned is not None
            or self.optional_alignment
            or self.split_dummy
            or self.order_lambda != 0.0
        )

        if special_sinkhorn:
            assert self.alignment == "sinkhorn"

        assert (
            self.max_num_aligned is None or self.one_to_k is None
        )  # only one at a time

        has_dummy = (
            self.max_num_aligned is not None
            or self.one_to_k is not None
            or self.optional_alignment
        )

        if self.split_dummy:
            assert has_dummy

        if self.optional_alignment:
            assert self.cost_fn in [
                "dot_product",
                "scaled_dot_product",
                "cosine_similarity",
            ]

        if self.max_num_aligned is not None:
            assert not self.optional_alignment
            assert self.max_num_aligned > 0

        if self.one_to_k is not None:
            assert self.one_to_k == -1 or self.one_to_k > 0


class AskUbuntuArguments(SharedArguments):
    """https://github.com/taolei87/askubuntu (viewed 09/05/2019)"""

    task: str = "similarity"

    # Data arguments
    text_path: str = "data/similarity/askubuntu/text_tokenized.txt"  # Path to .txt file contining the tokenized text
    train_path: str = "data/similarity/askubuntu/train_random.txt"  # Path to .txt file containing the train similar/dissimilar IDs
    dev_path: str = "data/similarity/askubuntu/dev.txt"  # Path to .txt file containing the dev similar/dissimilar IDs
    test_path: str = "data/similarity/askubuntu/test.txt"  # Path to .txt file containing the test similar/dissimilar IDs

    # Training arguments
    title_only: bool = False  # Whether to only use the title and not the body text
    batch_size: int = 20


class SuperUserAskUbuntuArguments(AskUbuntuArguments):
    """https://archive.org/details/stackexchange June 2019 dump"""

    task: str = "similarity"

    # Data arguments
    text_path: str = "data/similarity/superuser_askubuntu/text_tokenized.txt"
    train_path: str = "data/similarity/superuser_askubuntu/similar_pairs.txt"
    dev_path: str = None
    test_path: str = None

    # Training arguments
    num_eval_negatives: int = 20


class MultiNewsArguments(SharedArguments):
    """https://github.com/Alex-Fabbri/Multi-News (viewed 09/05/2019)"""

    task = "similarity"

    # Data arguments
    news_path: str = "data/similarity/multi-news-original/train.src"  # Path to .txt file containing articles

    # Training arguments
    batch_size: int = 10
    num_negatives: int = 10
    num_eval_negatives: int = 10


class SNLIArguments(SharedArguments):
    task: str = "classify"
    loss_fn: str = "cross_entropy"
    batch_size: int = 400

    # Data arguments
    snli_path: str = "data/classify/esnli/docs.jsonl"  # Path to .txt file containing articles
    word_to_word: bool = True
    bert = False


class MultircArguments(SharedArguments):
    task: str = "classify"
    loss_fn: str = "cross_entropy"
    batch_size: int = 200

    # Data arguments
    multirc_path: str = "data/classify/multirc"  # Path to .txt file containing articles


Arguments = Union[
    AskUbuntuArguments, MultiNewsArguments, SNLIArguments, MultircArguments,
]


def parse_dataset_args(
    dataset: str, extra_args: Optional[List[str]] = None
) -> Arguments:
    if dataset == "superuser_askubuntu":
        return SuperUserAskUbuntuArguments().parse_args(args=extra_args)
    elif dataset == "multinews":
        return MultiNewsArguments().parse_args(args=extra_args)
    elif dataset == "snli":
        return SNLIArguments().parse_args(args=extra_args)
    elif dataset == "multirc":
        return MultircArguments().parse_args(args=extra_args)

    raise ValueError(f'Dataset "{dataset}" not supported')


def parse_args() -> Arguments:
    dataset_args = DatasetArguments().parse_args(known_only=True)
    extra_args = ["--dataset", dataset_args.dataset] + dataset_args.extra_args
    args = parse_dataset_args(dataset=dataset_args.dataset, extra_args=extra_args)

    return args
